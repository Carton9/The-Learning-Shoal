{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.\n",
    "\n",
    "This code is exceedingly basic, with no logging or weights saving.\n",
    "The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.\n",
    "\n",
    "Author: Jet (https://github.com/jjshoots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            self._layer_init(nn.Linear(128 * 8 * 8, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # transpose to be (batch, channel, height, width)\n",
    "    obs = obs.transpose(0, -1, 1, 2)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 0\n",
      "Episodic Return: -8.605709075927734\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 3481.42578125\n",
      "Policy Loss: -81.28341674804688\n",
      "Old Approx KL: 0.006331436336040497\n",
      "Approx KL: 0.0005861856043338776\n",
      "Clip Fraction: 0.11979166666666667\n",
      "Explained Variance: 0.0014474987983703613\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1\n",
      "Episodic Return: -6.983731746673584\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 4.80156946182251\n",
      "Policy Loss: 2.466113567352295\n",
      "Old Approx KL: 0.0028492584824562073\n",
      "Approx KL: 0.0001297593116760254\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.00334775447845459\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 2\n",
      "Episodic Return: -19.386262893676758\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 8.604218482971191\n",
      "Policy Loss: -4.0236406326293945\n",
      "Old Approx KL: -0.0011630430817604065\n",
      "Approx KL: 4.227086901664734e-05\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.015670061111450195\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 3\n",
      "Episodic Return: 5.642217636108398\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 19.74547576904297\n",
      "Policy Loss: 5.278038501739502\n",
      "Old Approx KL: 0.028545551002025604\n",
      "Approx KL: 0.003747846931219101\n",
      "Clip Fraction: 0.0037393162393162395\n",
      "Explained Variance: 0.0033898353576660156\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 4\n",
      "Episodic Return: -10.760866165161133\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 9.731341361999512\n",
      "Policy Loss: 1.3226408958435059\n",
      "Old Approx KL: 0.011709511280059814\n",
      "Approx KL: 0.0009291023015975952\n",
      "Clip Fraction: 0.018963675213675212\n",
      "Explained Variance: -0.0027076005935668945\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 5\n",
      "Episodic Return: -17.92523765563965\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 2.0692148208618164\n",
      "Policy Loss: -1.1154592037200928\n",
      "Old Approx KL: -0.006687350571155548\n",
      "Approx KL: 0.0026624836027622223\n",
      "Clip Fraction: 0.011217948717948718\n",
      "Explained Variance: -0.025315523147583008\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 6\n",
      "Episodic Return: -0.6818181276321411\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 7.359549045562744\n",
      "Policy Loss: -3.094510316848755\n",
      "Old Approx KL: 0.00865986943244934\n",
      "Approx KL: 0.0022912435233592987\n",
      "Clip Fraction: 0.015892094017094016\n",
      "Explained Variance: 0.009075760841369629\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 7\n",
      "Episodic Return: 28.151540756225586\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 30.01811408996582\n",
      "Policy Loss: -4.524929523468018\n",
      "Old Approx KL: -0.008004114031791687\n",
      "Approx KL: 0.0010111182928085327\n",
      "Clip Fraction: 0.03579059829059829\n",
      "Explained Variance: -0.005533814430236816\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 8\n",
      "Episodic Return: -12.78611946105957\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 43.46443176269531\n",
      "Policy Loss: 8.538939476013184\n",
      "Old Approx KL: 0.007784601300954819\n",
      "Approx KL: 0.0007838979363441467\n",
      "Clip Fraction: 0.029513888888888888\n",
      "Explained Variance: -0.006467223167419434\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 9\n",
      "Episodic Return: -10.274491310119629\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 14.595739364624023\n",
      "Policy Loss: -5.003835678100586\n",
      "Old Approx KL: -0.00413639098405838\n",
      "Approx KL: 0.0005231164395809174\n",
      "Clip Fraction: 0.023237179487179488\n",
      "Explained Variance: -0.009504199028015137\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 10\n",
      "Episodic Return: 32.5\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 6.3647565841674805\n",
      "Policy Loss: -0.9591704607009888\n",
      "Old Approx KL: 0.007455214858055115\n",
      "Approx KL: 0.001236606389284134\n",
      "Clip Fraction: 0.03485576923076923\n",
      "Explained Variance: -0.02385890483856201\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 11\n",
      "Episodic Return: 24.31253433227539\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 30.6336669921875\n",
      "Policy Loss: 7.665571689605713\n",
      "Old Approx KL: 0.00369967520236969\n",
      "Approx KL: 7.603690028190613e-05\n",
      "Clip Fraction: 0.0109508547008547\n",
      "Explained Variance: 0.003274857997894287\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 12\n",
      "Episodic Return: -20.62030792236328\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 10.404801368713379\n",
      "Policy Loss: 2.9116902351379395\n",
      "Old Approx KL: -0.00044590234756469727\n",
      "Approx KL: 0.0017083100974559784\n",
      "Clip Fraction: 0.007745726495726496\n",
      "Explained Variance: -0.008961915969848633\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 13\n",
      "Episodic Return: -12.778955459594727\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 3.4258921146392822\n",
      "Policy Loss: -2.565495729446411\n",
      "Old Approx KL: -0.0002032443881034851\n",
      "Approx KL: 0.0003102831542491913\n",
      "Clip Fraction: 0.039797008547008544\n",
      "Explained Variance: -0.04339337348937988\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 14\n",
      "Episodic Return: 20.97762107849121\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 5.69346809387207\n",
      "Policy Loss: -2.1343917846679688\n",
      "Old Approx KL: -0.012467272579669952\n",
      "Approx KL: 0.0014170855283737183\n",
      "Clip Fraction: 0.05515491452991453\n",
      "Explained Variance: 0.007282555103302002\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 15\n",
      "Episodic Return: -9.999998092651367\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 26.30658721923828\n",
      "Policy Loss: 6.720578193664551\n",
      "Old Approx KL: 0.010405406355857849\n",
      "Approx KL: 0.0003492385149002075\n",
      "Clip Fraction: 0.05742521367521367\n",
      "Explained Variance: -0.026496529579162598\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 16\n",
      "Episodic Return: 7.853971004486084\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 9.323807716369629\n",
      "Policy Loss: -2.3969340324401855\n",
      "Old Approx KL: 0.004144437611103058\n",
      "Approx KL: 0.0007919222116470337\n",
      "Clip Fraction: 0.05515491452991453\n",
      "Explained Variance: -0.0007877349853515625\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 17\n",
      "Episodic Return: -13.934724807739258\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 51.945823669433594\n",
      "Policy Loss: 9.745505332946777\n",
      "Old Approx KL: -0.0064456164836883545\n",
      "Approx KL: 0.0009935647249221802\n",
      "Clip Fraction: 0.07291666666666667\n",
      "Explained Variance: -0.008459091186523438\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 18\n",
      "Episodic Return: 2.071430206298828\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 16.703535079956055\n",
      "Policy Loss: -1.4134235382080078\n",
      "Old Approx KL: -0.03290838748216629\n",
      "Approx KL: 0.001018717885017395\n",
      "Clip Fraction: 0.03125\n",
      "Explained Variance: 0.0011584162712097168\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 19\n",
      "Episodic Return: -13.502866744995117\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 35.55147933959961\n",
      "Policy Loss: 7.343733310699463\n",
      "Old Approx KL: -0.02216806262731552\n",
      "Approx KL: 0.003297511488199234\n",
      "Clip Fraction: 0.11298076923076923\n",
      "Explained Variance: 0.0023812055587768555\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 20\n",
      "Episodic Return: -14.231607437133789\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 2.2311856746673584\n",
      "Policy Loss: 0.9029175043106079\n",
      "Old Approx KL: -0.0018570348620414734\n",
      "Approx KL: 7.110461592674255e-05\n",
      "Clip Fraction: 0.01014957264957265\n",
      "Explained Variance: -0.05035901069641113\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 21\n",
      "Episodic Return: -9.278712272644043\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 13.823869705200195\n",
      "Policy Loss: -2.8433780670166016\n",
      "Old Approx KL: -0.004621319472789764\n",
      "Approx KL: 0.00012569501996040344\n",
      "Clip Fraction: 0.028311965811965812\n",
      "Explained Variance: 0.0011389851570129395\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 22\n",
      "Episodic Return: -16.672908782958984\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 4.571490287780762\n",
      "Policy Loss: 0.5303125381469727\n",
      "Old Approx KL: 0.006172358989715576\n",
      "Approx KL: 0.0005727000534534454\n",
      "Clip Fraction: 0.047275641025641024\n",
      "Explained Variance: 0.0013338923454284668\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 23\n",
      "Episodic Return: 81.33462524414062\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 163.75189208984375\n",
      "Policy Loss: -14.760981559753418\n",
      "Old Approx KL: -0.019174892455339432\n",
      "Approx KL: 0.001494191586971283\n",
      "Clip Fraction: 0.12206196581196581\n",
      "Explained Variance: 0.002608776092529297\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 24\n",
      "Episodic Return: -11.063215255737305\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 0.4672192633152008\n",
      "Policy Loss: 0.21188034117221832\n",
      "Old Approx KL: -0.010701261460781097\n",
      "Approx KL: 0.00011305510997772217\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.016374707221984863\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 25\n",
      "Episodic Return: -6.626084804534912\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 15.017644882202148\n",
      "Policy Loss: -4.130842208862305\n",
      "Old Approx KL: 0.012002669274806976\n",
      "Approx KL: 0.0006217807531356812\n",
      "Clip Fraction: 0.011485042735042736\n",
      "Explained Variance: 0.00017976760864257812\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 26\n",
      "Episodic Return: -6.240905284881592\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 3.4726998805999756\n",
      "Policy Loss: 1.145437479019165\n",
      "Old Approx KL: 0.010326560586690903\n",
      "Approx KL: 0.0011720843613147736\n",
      "Clip Fraction: 0.005608974358974359\n",
      "Explained Variance: -0.002783060073852539\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 27\n",
      "Episodic Return: -16.251808166503906\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 15.332647323608398\n",
      "Policy Loss: 4.465344429016113\n",
      "Old Approx KL: -0.0002668052911758423\n",
      "Approx KL: 0.0027862973511219025\n",
      "Clip Fraction: 0.07585470085470085\n",
      "Explained Variance: 0.0035060644149780273\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 28\n",
      "Episodic Return: -9.474279403686523\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 51.75457763671875\n",
      "Policy Loss: -8.826581954956055\n",
      "Old Approx KL: 0.023766379803419113\n",
      "Approx KL: 0.00404251366853714\n",
      "Clip Fraction: 0.1424946581196581\n",
      "Explained Variance: -0.0007129907608032227\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 29\n",
      "Episodic Return: 3.8865561485290527\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 12.395395278930664\n",
      "Policy Loss: -4.140838623046875\n",
      "Old Approx KL: -0.012919899076223373\n",
      "Approx KL: 0.0008401200175285339\n",
      "Clip Fraction: 0.04126602564102564\n",
      "Explained Variance: 0.003148496150970459\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 30\n",
      "Episodic Return: 9.626420974731445\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 17.295852661132812\n",
      "Policy Loss: -2.162613868713379\n",
      "Old Approx KL: -0.02558216080069542\n",
      "Approx KL: 0.0017907284200191498\n",
      "Clip Fraction: 0.08800747863247864\n",
      "Explained Variance: -0.010168790817260742\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 31\n",
      "Episodic Return: -6.756257057189941\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 22.76737403869629\n",
      "Policy Loss: 6.33338737487793\n",
      "Old Approx KL: -0.005701683461666107\n",
      "Approx KL: 0.002056848257780075\n",
      "Clip Fraction: 0.1390224358974359\n",
      "Explained Variance: -0.007946014404296875\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 32\n",
      "Episodic Return: -11.795774459838867\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 2.5239322185516357\n",
      "Policy Loss: 2.0363259315490723\n",
      "Old Approx KL: -0.015236962586641312\n",
      "Approx KL: 0.0041184090077877045\n",
      "Clip Fraction: 0.12566773504273504\n",
      "Explained Variance: -0.021380066871643066\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 33\n",
      "Episodic Return: 7.731215000152588\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 73.77828216552734\n",
      "Policy Loss: -11.330815315246582\n",
      "Old Approx KL: -0.021203219890594482\n",
      "Approx KL: 0.004225924611091614\n",
      "Clip Fraction: 0.21461004273504272\n",
      "Explained Variance: -0.007471919059753418\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 34\n",
      "Episodic Return: 35.155067443847656\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 31.77589988708496\n",
      "Policy Loss: -4.524787425994873\n",
      "Old Approx KL: -0.0027101747691631317\n",
      "Approx KL: 0.003785114735364914\n",
      "Clip Fraction: 0.08827457264957266\n",
      "Explained Variance: 0.000910341739654541\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 35\n",
      "Episodic Return: -13.957746505737305\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 48.66775894165039\n",
      "Policy Loss: 9.53650188446045\n",
      "Old Approx KL: 0.008249741047620773\n",
      "Approx KL: 0.0037098973989486694\n",
      "Clip Fraction: 0.1673344017094017\n",
      "Explained Variance: -0.004241585731506348\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 36\n",
      "Episodic Return: -9.57602310180664\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 1.9449074268341064\n",
      "Policy Loss: 1.1678425073623657\n",
      "Old Approx KL: 0.0006719790399074554\n",
      "Approx KL: 0.002438213676214218\n",
      "Clip Fraction: 0.03205128205128205\n",
      "Explained Variance: -0.001214742660522461\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 37\n",
      "Episodic Return: -9.361627578735352\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 6.418663501739502\n",
      "Policy Loss: -3.1412668228149414\n",
      "Old Approx KL: -0.010725434869527817\n",
      "Approx KL: 0.0016793496906757355\n",
      "Clip Fraction: 0.07852564102564102\n",
      "Explained Variance: -0.007353305816650391\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 38\n",
      "Episodic Return: 15.222749710083008\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 13.023763656616211\n",
      "Policy Loss: 3.8761401176452637\n",
      "Old Approx KL: 0.012784443795681\n",
      "Approx KL: 0.0025998875498771667\n",
      "Clip Fraction: 0.09909188034188034\n",
      "Explained Variance: 0.005853593349456787\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 39\n",
      "Episodic Return: 77.9478530883789\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 106.64878845214844\n",
      "Policy Loss: -12.718973159790039\n",
      "Old Approx KL: 0.023459266871213913\n",
      "Approx KL: 0.005340132862329483\n",
      "Clip Fraction: 0.21300747863247863\n",
      "Explained Variance: 0.009725391864776611\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 40\n",
      "Episodic Return: 42.58100509643555\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 94.4592056274414\n",
      "Policy Loss: -10.6903076171875\n",
      "Old Approx KL: -0.03163754940032959\n",
      "Approx KL: 0.0022639892995357513\n",
      "Clip Fraction: 0.12620192307692307\n",
      "Explained Variance: -0.014410018920898438\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 41\n",
      "Episodic Return: 28.261621475219727\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 49.419769287109375\n",
      "Policy Loss: 6.6119279861450195\n",
      "Old Approx KL: 0.0010052323341369629\n",
      "Approx KL: 0.001063130795955658\n",
      "Clip Fraction: 0.11698717948717949\n",
      "Explained Variance: -0.002290487289428711\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 42\n",
      "Episodic Return: 5.0616865158081055\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 33.85670471191406\n",
      "Policy Loss: 5.34787654876709\n",
      "Old Approx KL: -0.022087357938289642\n",
      "Approx KL: 0.004799686372280121\n",
      "Clip Fraction: 0.15985576923076922\n",
      "Explained Variance: -0.005499362945556641\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 43\n",
      "Episodic Return: -1.8217853307724\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 3.63909912109375\n",
      "Policy Loss: 0.9630845785140991\n",
      "Old Approx KL: 0.00430934876203537\n",
      "Approx KL: 0.0011277124285697937\n",
      "Clip Fraction: 0.11364850427350427\n",
      "Explained Variance: -0.014284968376159668\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 44\n",
      "Episodic Return: 42.4708137512207\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 30.523212432861328\n",
      "Policy Loss: -1.0713310241699219\n",
      "Old Approx KL: 0.0036622658371925354\n",
      "Approx KL: 0.0032627470791339874\n",
      "Clip Fraction: 0.1856303418803419\n",
      "Explained Variance: 0.005862236022949219\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 45\n",
      "Episodic Return: 11.866195678710938\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 31.648414611816406\n",
      "Policy Loss: 2.714022636413574\n",
      "Old Approx KL: -0.003033943474292755\n",
      "Approx KL: 0.005105629563331604\n",
      "Clip Fraction: 0.19404380341880342\n",
      "Explained Variance: 0.005472242832183838\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 46\n",
      "Episodic Return: 45.084712982177734\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 136.29078674316406\n",
      "Policy Loss: -13.853730201721191\n",
      "Old Approx KL: -0.013807915151119232\n",
      "Approx KL: 0.00305943563580513\n",
      "Clip Fraction: 0.24185363247863248\n",
      "Explained Variance: 0.005588054656982422\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 47\n",
      "Episodic Return: 60.8038444519043\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 117.71788787841797\n",
      "Policy Loss: -13.671424865722656\n",
      "Old Approx KL: 0.015962403267621994\n",
      "Approx KL: 0.004851020872592926\n",
      "Clip Fraction: 0.19404380341880342\n",
      "Explained Variance: -0.01806807518005371\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 48\n",
      "Episodic Return: 46.565147399902344\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 66.34786987304688\n",
      "Policy Loss: 9.367992401123047\n",
      "Old Approx KL: 0.01807244122028351\n",
      "Approx KL: 0.00397084653377533\n",
      "Clip Fraction: 0.23517628205128205\n",
      "Explained Variance: -0.02184438705444336\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 49\n",
      "Episodic Return: 12.7124605178833\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 18.154571533203125\n",
      "Policy Loss: 4.736529350280762\n",
      "Old Approx KL: 0.004566743969917297\n",
      "Approx KL: 0.0034108497202396393\n",
      "Clip Fraction: 0.14623397435897437\n",
      "Explained Variance: -0.010855913162231445\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 50\n",
      "Episodic Return: 29.51438331604004\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 15.01302433013916\n",
      "Policy Loss: -3.486927032470703\n",
      "Old Approx KL: 0.005682151764631271\n",
      "Approx KL: 0.001719936728477478\n",
      "Clip Fraction: 0.15104166666666666\n",
      "Explained Variance: 0.006462752819061279\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 51\n",
      "Episodic Return: 25.19559097290039\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 6.271604537963867\n",
      "Policy Loss: -1.5317132472991943\n",
      "Old Approx KL: -0.005769498646259308\n",
      "Approx KL: 0.0012831948697566986\n",
      "Clip Fraction: 0.19017094017094016\n",
      "Explained Variance: -0.012094855308532715\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 52\n",
      "Episodic Return: 56.494407653808594\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 9.848052024841309\n",
      "Policy Loss: -2.442564010620117\n",
      "Old Approx KL: -0.022733882069587708\n",
      "Approx KL: 0.0021628662943840027\n",
      "Clip Fraction: 0.1685363247863248\n",
      "Explained Variance: -0.0001163482666015625\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 53\n",
      "Episodic Return: 74.26260375976562\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 32.371700286865234\n",
      "Policy Loss: -7.741861820220947\n",
      "Old Approx KL: -0.044573668390512466\n",
      "Approx KL: 0.003932550549507141\n",
      "Clip Fraction: 0.29286858974358976\n",
      "Explained Variance: 0.0015901923179626465\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 54\n",
      "Episodic Return: 32.25218200683594\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 154.75579833984375\n",
      "Policy Loss: 15.211833000183105\n",
      "Old Approx KL: 0.00978214293718338\n",
      "Approx KL: 0.04582373797893524\n",
      "Clip Fraction: 0.3994391025641026\n",
      "Explained Variance: -0.0317002534866333\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 55\n",
      "Episodic Return: 17.19188690185547\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 10.441215515136719\n",
      "Policy Loss: 2.0837085247039795\n",
      "Old Approx KL: 0.046752799302339554\n",
      "Approx KL: 0.005348037928342819\n",
      "Clip Fraction: 0.21501068376068377\n",
      "Explained Variance: -0.011030793190002441\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 56\n",
      "Episodic Return: 44.66272735595703\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 51.160125732421875\n",
      "Policy Loss: -8.955856323242188\n",
      "Old Approx KL: -0.046127501875162125\n",
      "Approx KL: 0.0062145330011844635\n",
      "Clip Fraction: 0.2799145299145299\n",
      "Explained Variance: -0.001223921775817871\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 57\n",
      "Episodic Return: 29.440082550048828\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 13.932764053344727\n",
      "Policy Loss: 3.6377499103546143\n",
      "Old Approx KL: 0.0459880530834198\n",
      "Approx KL: 0.004640340805053711\n",
      "Clip Fraction: 0.2065972222222222\n",
      "Explained Variance: 0.0023300647735595703\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 58\n",
      "Episodic Return: 88.80004119873047\n",
      "Episode Length: 112\n",
      "\n",
      "Value Loss: 69.64437866210938\n",
      "Policy Loss: -9.076918601989746\n",
      "Old Approx KL: -0.057937927544116974\n",
      "Approx KL: 0.0075844693928956985\n",
      "Clip Fraction: 0.3132440476190476\n",
      "Explained Variance: 0.021007120609283447\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 59\n",
      "Episodic Return: 68.81391906738281\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 53.329124450683594\n",
      "Policy Loss: -8.9014892578125\n",
      "Old Approx KL: 0.006483662873506546\n",
      "Approx KL: 0.005173705518245697\n",
      "Clip Fraction: 0.30435363247863245\n",
      "Explained Variance: 0.005861341953277588\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 60\n",
      "Episodic Return: 65.30944061279297\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 6.864679336547852\n",
      "Policy Loss: 1.950440526008606\n",
      "Old Approx KL: 0.0014493055641651154\n",
      "Approx KL: 0.0022729896008968353\n",
      "Clip Fraction: 0.2423878205128205\n",
      "Explained Variance: 0.0190393328666687\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 61\n",
      "Episodic Return: 79.12931060791016\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 23.019075393676758\n",
      "Policy Loss: -4.696658611297607\n",
      "Old Approx KL: -0.02982843667268753\n",
      "Approx KL: 0.003557726740837097\n",
      "Clip Fraction: 0.2857905982905983\n",
      "Explained Variance: -0.013355731964111328\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 62\n",
      "Episodic Return: 89.19998931884766\n",
      "Episode Length: 108\n",
      "\n",
      "Value Loss: 55.638427734375\n",
      "Policy Loss: -10.465073585510254\n",
      "Old Approx KL: -0.04066421836614609\n",
      "Approx KL: 0.0023679621517658234\n",
      "Clip Fraction: 0.3330269607843137\n",
      "Explained Variance: -0.07564842700958252\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 63\n",
      "Episodic Return: -0.3787834048271179\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 461.21343994140625\n",
      "Policy Loss: 30.8016357421875\n",
      "Old Approx KL: -0.0028295740485191345\n",
      "Approx KL: 0.046059444546699524\n",
      "Clip Fraction: 0.5599626068376068\n",
      "Explained Variance: -0.016039133071899414\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 64\n",
      "Episodic Return: 90.80007934570312\n",
      "Episode Length: 92\n",
      "\n",
      "Value Loss: 7.324254035949707\n",
      "Policy Loss: -3.2032923698425293\n",
      "Old Approx KL: -0.015542890876531601\n",
      "Approx KL: 0.0015889666974544525\n",
      "Clip Fraction: 0.20581896551724138\n",
      "Explained Variance: 0.01758253574371338\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 65\n",
      "Episodic Return: 89.10005187988281\n",
      "Episode Length: 109\n",
      "\n",
      "Value Loss: 12.871633529663086\n",
      "Policy Loss: -2.7432355880737305\n",
      "Old Approx KL: 0.021718263626098633\n",
      "Approx KL: 0.0016908347606658936\n",
      "Clip Fraction: 0.16576086956521738\n",
      "Explained Variance: 0.007514476776123047\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 66\n",
      "Episodic Return: 88.80003356933594\n",
      "Episode Length: 112\n",
      "\n",
      "Value Loss: 9.622361183166504\n",
      "Policy Loss: -2.802356719970703\n",
      "Old Approx KL: -0.004730576649308205\n",
      "Approx KL: 0.0033147279173135757\n",
      "Clip Fraction: 0.18854166666666666\n",
      "Explained Variance: 0.00022161006927490234\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 67\n",
      "Episodic Return: 45.70032501220703\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 64.98308563232422\n",
      "Policy Loss: 10.463388442993164\n",
      "Old Approx KL: 0.050324954092502594\n",
      "Approx KL: 0.0032802000641822815\n",
      "Clip Fraction: 0.38995726495726496\n",
      "Explained Variance: -0.04775655269622803\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 68\n",
      "Episodic Return: 30.417247772216797\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 26.480087280273438\n",
      "Policy Loss: -2.2715156078338623\n",
      "Old Approx KL: 0.026865657418966293\n",
      "Approx KL: 0.0032733194530010223\n",
      "Clip Fraction: 0.28205128205128205\n",
      "Explained Variance: -0.010652303695678711\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 69\n",
      "Episodic Return: 41.35779571533203\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 23.3427734375\n",
      "Policy Loss: -3.5222573280334473\n",
      "Old Approx KL: -0.04153463989496231\n",
      "Approx KL: 0.008758455514907837\n",
      "Clip Fraction: 0.2016559829059829\n",
      "Explained Variance: 0.0009867548942565918\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 70\n",
      "Episodic Return: 88.10002136230469\n",
      "Episode Length: 119\n",
      "\n",
      "Value Loss: 163.59976196289062\n",
      "Policy Loss: -18.202632904052734\n",
      "Old Approx KL: -0.05241481959819794\n",
      "Approx KL: 0.0036860108375549316\n",
      "Clip Fraction: 0.43986111137602063\n",
      "Explained Variance: 0.003913760185241699\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 71\n",
      "Episodic Return: 87.35737609863281\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 15.309107780456543\n",
      "Policy Loss: 2.567218065261841\n",
      "Old Approx KL: 0.04860673472285271\n",
      "Approx KL: 0.00345606729388237\n",
      "Clip Fraction: 0.25093482905982906\n",
      "Explained Variance: -0.02402520179748535\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 72\n",
      "Episodic Return: 56.29433059692383\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 27.418560028076172\n",
      "Policy Loss: 5.836198806762695\n",
      "Old Approx KL: 0.06905484199523926\n",
      "Approx KL: 0.012845337390899658\n",
      "Clip Fraction: 0.3802083333333333\n",
      "Explained Variance: -0.010921835899353027\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 73\n",
      "Episodic Return: 51.20262908935547\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 35.649105072021484\n",
      "Policy Loss: -3.9057116508483887\n",
      "Old Approx KL: -0.04029315337538719\n",
      "Approx KL: 0.004454758018255234\n",
      "Clip Fraction: 0.27337072649572647\n",
      "Explained Variance: 0.006164848804473877\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 74\n",
      "Episodic Return: 68.84110260009766\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 68.58853912353516\n",
      "Policy Loss: -10.523452758789062\n",
      "Old Approx KL: -0.05858263373374939\n",
      "Approx KL: 0.007924884557723999\n",
      "Clip Fraction: 0.37553418803418803\n",
      "Explained Variance: 0.0020900964736938477\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 75\n",
      "Episodic Return: 89.40003204345703\n",
      "Episode Length: 106\n",
      "\n",
      "Value Loss: 36.75144958496094\n",
      "Policy Loss: -6.0697503089904785\n",
      "Old Approx KL: 0.019031360745429993\n",
      "Approx KL: 0.012557178735733032\n",
      "Clip Fraction: 0.36411691542288555\n",
      "Explained Variance: -0.005859375\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 76\n",
      "Episodic Return: 25.860164642333984\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 262.14093017578125\n",
      "Policy Loss: 21.773948669433594\n",
      "Old Approx KL: 0.07199671864509583\n",
      "Approx KL: 0.014304082840681076\n",
      "Clip Fraction: 0.5665064102564102\n",
      "Explained Variance: -0.019098877906799316\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 77\n",
      "Episodic Return: 87.800048828125\n",
      "Episode Length: 122\n",
      "\n",
      "Value Loss: 128.8343505859375\n",
      "Policy Loss: -14.053836822509766\n",
      "Old Approx KL: 0.04095747321844101\n",
      "Approx KL: 0.01755589246749878\n",
      "Clip Fraction: 0.3887987012987013\n",
      "Explained Variance: 0.0014585256576538086\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 78\n",
      "Episodic Return: 34.4101448059082\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 186.52560424804688\n",
      "Policy Loss: 17.08966636657715\n",
      "Old Approx KL: 0.050679098814725876\n",
      "Approx KL: 0.008249998092651367\n",
      "Clip Fraction: 0.5061431623931624\n",
      "Explained Variance: -0.0023627281188964844\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 79\n",
      "Episodic Return: 67.16575622558594\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 90.32147216796875\n",
      "Policy Loss: -12.339800834655762\n",
      "Old Approx KL: -0.04697567597031593\n",
      "Approx KL: 0.011907991021871567\n",
      "Clip Fraction: 0.33092948717948717\n",
      "Explained Variance: -0.01069343090057373\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 80\n",
      "Episodic Return: 62.575172424316406\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 85.26026916503906\n",
      "Policy Loss: 9.621954917907715\n",
      "Old Approx KL: 0.018208883702754974\n",
      "Approx KL: 0.007207304239273071\n",
      "Clip Fraction: 0.39289529914529914\n",
      "Explained Variance: -0.0024683475494384766\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 81\n",
      "Episodic Return: 44.62122344970703\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 12.593340873718262\n",
      "Policy Loss: -0.6198011636734009\n",
      "Old Approx KL: -0.011174838989973068\n",
      "Approx KL: 0.0071229487657547\n",
      "Clip Fraction: 0.19497863247863248\n",
      "Explained Variance: -0.021777868270874023\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 82\n",
      "Episodic Return: 39.93394088745117\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 10.946467399597168\n",
      "Policy Loss: 3.092407703399658\n",
      "Old Approx KL: 0.02628098800778389\n",
      "Approx KL: 0.00809602439403534\n",
      "Clip Fraction: 0.23357371794871795\n",
      "Explained Variance: -0.009018659591674805\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 83\n",
      "Episodic Return: 44.53594207763672\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 21.878387451171875\n",
      "Policy Loss: -4.346956253051758\n",
      "Old Approx KL: -0.03552673012018204\n",
      "Approx KL: 0.005997225642204285\n",
      "Clip Fraction: 0.2564102564102564\n",
      "Explained Variance: -0.0016858577728271484\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 84\n",
      "Episodic Return: 91.90001678466797\n",
      "Episode Length: 81\n",
      "\n",
      "Value Loss: 107.52043914794922\n",
      "Policy Loss: -9.265706062316895\n",
      "Old Approx KL: -0.015490055084228516\n",
      "Approx KL: 0.018078505992889404\n",
      "Clip Fraction: 0.431535947946162\n",
      "Explained Variance: -0.0025349855422973633\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 85\n",
      "Episodic Return: 88.80000305175781\n",
      "Episode Length: 112\n",
      "\n",
      "Value Loss: 73.91915893554688\n",
      "Policy Loss: -9.355849266052246\n",
      "Old Approx KL: -0.009120877832174301\n",
      "Approx KL: 0.013455228880047798\n",
      "Clip Fraction: 0.4108630952380952\n",
      "Explained Variance: -0.0010727643966674805\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 86\n",
      "Episodic Return: 91.20004272460938\n",
      "Episode Length: 88\n",
      "\n",
      "Value Loss: 80.0042953491211\n",
      "Policy Loss: -0.8239285945892334\n",
      "Old Approx KL: 0.04715563729405403\n",
      "Approx KL: 0.025509877130389214\n",
      "Clip Fraction: 0.43333333333333335\n",
      "Explained Variance: 0.00042694807052612305\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 87\n",
      "Episodic Return: 87.59996032714844\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 21.71702766418457\n",
      "Policy Loss: 4.8475542068481445\n",
      "Old Approx KL: 0.04509516805410385\n",
      "Approx KL: 0.0077231526374816895\n",
      "Clip Fraction: 0.4173344017094017\n",
      "Explained Variance: -0.0016952753067016602\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 88\n",
      "Episodic Return: 34.108943939208984\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 108.43675231933594\n",
      "Policy Loss: 13.177841186523438\n",
      "Old Approx KL: 0.08651654422283173\n",
      "Approx KL: 0.023822791874408722\n",
      "Clip Fraction: 0.48838141025641024\n",
      "Explained Variance: -0.003132462501525879\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 89\n",
      "Episodic Return: 46.43858337402344\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 36.9705924987793\n",
      "Policy Loss: -8.053653717041016\n",
      "Old Approx KL: -0.05122905597090721\n",
      "Approx KL: 0.0178278349339962\n",
      "Clip Fraction: 0.34615384615384615\n",
      "Explained Variance: -0.00971531867980957\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 90\n",
      "Episodic Return: 25.92795181274414\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 33.119773864746094\n",
      "Policy Loss: 5.685953617095947\n",
      "Old Approx KL: 0.13137850165367126\n",
      "Approx KL: 0.01599128544330597\n",
      "Clip Fraction: 0.43162393162393164\n",
      "Explained Variance: -0.0035431385040283203\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 91\n",
      "Episodic Return: 34.718265533447266\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 23.287002563476562\n",
      "Policy Loss: -2.46675443649292\n",
      "Old Approx KL: -0.04206695780158043\n",
      "Approx KL: 0.00449872761964798\n",
      "Clip Fraction: 0.34922542735042733\n",
      "Explained Variance: 0.00104600191116333\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 92\n",
      "Episodic Return: 88.7000503540039\n",
      "Episode Length: 113\n",
      "\n",
      "Value Loss: 53.606231689453125\n",
      "Policy Loss: -10.35472583770752\n",
      "Old Approx KL: -0.08424288034439087\n",
      "Approx KL: 0.010634279809892178\n",
      "Clip Fraction: 0.4720070423654547\n",
      "Explained Variance: 0.0007998943328857422\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 93\n",
      "Episodic Return: 60.334144592285156\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 58.09577941894531\n",
      "Policy Loss: 9.146856307983398\n",
      "Old Approx KL: 0.054327841848134995\n",
      "Approx KL: 0.01128481701016426\n",
      "Clip Fraction: 0.48210470085470086\n",
      "Explained Variance: -0.02439868450164795\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 94\n",
      "Episodic Return: 34.31758499145508\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 31.822099685668945\n",
      "Policy Loss: 1.4865466356277466\n",
      "Old Approx KL: 0.006990861147642136\n",
      "Approx KL: 0.0124099962413311\n",
      "Clip Fraction: 0.4081196581196581\n",
      "Explained Variance: 0.0034822821617126465\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 95\n",
      "Episodic Return: -6.651211738586426\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 17.768192291259766\n",
      "Policy Loss: 4.9634528160095215\n",
      "Old Approx KL: 0.048656120896339417\n",
      "Approx KL: 0.003456391394138336\n",
      "Clip Fraction: 0.3193108974358974\n",
      "Explained Variance: -0.03237271308898926\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 96\n",
      "Episodic Return: 20.021465301513672\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 31.374691009521484\n",
      "Policy Loss: -6.363365173339844\n",
      "Old Approx KL: -0.10679715871810913\n",
      "Approx KL: 0.011436078697443008\n",
      "Clip Fraction: 0.3138354700854701\n",
      "Explained Variance: -0.0076416730880737305\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 97\n",
      "Episodic Return: -17.905406951904297\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 74.40631103515625\n",
      "Policy Loss: 11.225892066955566\n",
      "Old Approx KL: 0.09083297848701477\n",
      "Approx KL: 0.017556477338075638\n",
      "Clip Fraction: 0.44190705128205127\n",
      "Explained Variance: -0.017136335372924805\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 98\n",
      "Episodic Return: 15.349932670593262\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 168.234619140625\n",
      "Policy Loss: -17.366214752197266\n",
      "Old Approx KL: -0.07700304687023163\n",
      "Approx KL: 0.011579733341932297\n",
      "Clip Fraction: 0.49225427350427353\n",
      "Explained Variance: 0.0038027167320251465\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 99\n",
      "Episodic Return: -12.647721290588379\n",
      "Episode Length: 124\n",
      "\n",
      "Value Loss: 45.24570846557617\n",
      "Policy Loss: 7.549220085144043\n",
      "Old Approx KL: 0.10998500883579254\n",
      "Approx KL: 0.020370468497276306\n",
      "Clip Fraction: 0.3349358974358974\n",
      "Explained Variance: -0.03780210018157959\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Displaying Episode:  0\n",
      "Displaying Episode:  1\n",
      "Displaying Episode:  2\n",
      "Displaying Episode:  3\n",
      "Displaying Episode:  4\n",
      "Displaying Episode:  5\n",
      "Displaying Episode:  6\n",
      "Displaying Episode:  7\n",
      "Displaying Episode:  8\n",
      "Displaying Episode:  9\n",
      "Displaying Episode:  10\n",
      "Displaying Episode:  11\n",
      "Displaying Episode:  12\n",
      "Displaying Episode:  13\n",
      "Displaying Episode:  14\n",
      "Displaying Episode:  15\n",
      "Displaying Episode:  16\n",
      "Displaying Episode:  17\n",
      "Displaying Episode:  18\n",
      "Displaying Episode:  19\n",
      "Displaying Episode:  20\n",
      "Displaying Episode:  21\n",
      "Displaying Episode:  22\n",
      "Displaying Episode:  23\n",
      "Displaying Episode:  24\n",
      "Displaying Episode:  25\n",
      "Displaying Episode:  26\n",
      "Displaying Episode:  27\n",
      "Displaying Episode:  28\n",
      "Displaying Episode:  29\n",
      "Displaying Episode:  30\n",
      "Displaying Episode:  31\n",
      "Displaying Episode:  32\n",
      "Displaying Episode:  33\n",
      "Displaying Episode:  34\n",
      "Displaying Episode:  35\n",
      "Displaying Episode:  36\n",
      "Displaying Episode:  37\n",
      "Displaying Episode:  38\n",
      "Displaying Episode:  39\n",
      "Displaying Episode:  40\n",
      "Displaying Episode:  41\n",
      "Displaying Episode:  42\n",
      "Displaying Episode:  43\n",
      "Displaying Episode:  44\n",
      "Displaying Episode:  45\n",
      "Displaying Episode:  46\n",
      "Displaying Episode:  47\n",
      "Displaying Episode:  48\n",
      "Displaying Episode:  49\n",
      "Displaying Episode:  50\n",
      "Displaying Episode:  51\n",
      "Displaying Episode:  52\n",
      "Displaying Episode:  53\n",
      "Displaying Episode:  54\n",
      "Displaying Episode:  55\n",
      "Displaying Episode:  56\n",
      "Displaying Episode:  57\n",
      "Displaying Episode:  58\n",
      "Displaying Episode:  59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8892\\1857963954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mtruncs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action_and_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munbatchify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchify_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8892\\2981538257.py\u001b[0m in \u001b[0;36mget_action_and_value\u001b[1;34m(self, x, action)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                     raise ValueError(\n\u001b[0;32m     57\u001b[0m                         \u001b[1;34mf\"Expected parameter {param} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    stack_size = 4\n",
    "    frame_size = (64, 64)\n",
    "    max_cycles = 125\n",
    "    total_episodes = 100\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles\n",
    "    )\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, frame_size[0], frame_size[1])\n",
    "    env = frame_stack_v1(env, stack_size=stack_size)\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    # train for n number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # collect observations and convert to batch of torch tensors\n",
    "            next_obs = env.reset(seed=None)\n",
    "            # reset the episodic return\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "\n",
    "                # rollover the observation\n",
    "                obs = batchify_obs(next_obs, device)\n",
    "\n",
    "                # get action from the agent\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "\n",
    "                # execute the environment and log data\n",
    "                next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                    unbatchify(actions, env)\n",
    "                )\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = obs\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                # compute episodic return\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    end_step = step\n",
    "                    break\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            for t in reversed(range(end_step)):\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        # convert our episodes to batch of individual transitions\n",
    "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(3):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantaegs\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    \"\"\" RENDER THE POLICY \"\"\"\n",
    "    env = pistonball_v6.parallel_env(render_mode=\"human\", continuous=False)\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, 64, 64)\n",
    "    env = frame_stack_v1(env, stack_size=4)\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # render 5 episodes out\n",
    "        for episode in range(5):\n",
    "            print(\"Displaying Episode: \", episode)\n",
    "            obs = batchify_obs(env.reset(seed=None), device)\n",
    "            terms = [False]\n",
    "            truncs = [False]\n",
    "            while not any(terms) and not any(truncs):\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "                obs = batchify_obs(obs, device)\n",
    "                terms = [terms[a] for a in terms]\n",
    "                truncs = [truncs[a] for a in truncs]\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
