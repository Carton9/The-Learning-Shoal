{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4, adversarial_pursuit_v4, tiger_deer_v4\n",
    "import sys, os\n",
    "from pettingzoo.utils import random_demo\n",
    "import pygame\n",
    "import torch\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from dqn_basic import DQN_Basic\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = 'cpu'\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 4\n",
    "frame_size = (128, 128)\n",
    "max_cycles = 100\n",
    "total_episodes = 100\n",
    "map_size = 30\n",
    "exploration_rate=0.2 #[0,1]\n",
    "\n",
    "env = tiger_deer_v4.env(map_size=map_size, minimap_mode=False, render_mode='rgb_array', tiger_step_recover=-0.1, deer_attacked=-0.1, max_cycles=max_cycles, extra_features=False)\n",
    "\n",
    "# random_demo(env, render=False , episodes=1)\n",
    "# 'rgb_array'\n",
    "# pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiger_DQN_Basic(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(Tiger_DQN_Basic, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deer_DQN_Basic(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(Deer_DQN_Basic, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deer_policy(model, observation, prev_observation, no_previous=True):\n",
    "    global exploration_rate, device\n",
    "    observation.to(device=device)\n",
    "    \n",
    "    if no_previous:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    # return torch.randint(0,model.n_actions, (1,))\n",
    "    # stacked_observations = torch.cat((observation, prev_observation), 1)\n",
    "    # Make a prediction based on the stacked observations\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    q_values = model.forward(observation)    \n",
    "    # Choose the action with the highest Q-value\n",
    "    action = torch.argmax(q_values)\n",
    "    # print(type(observation))\n",
    "    # print(observation.shape)\n",
    "    # 0 - up\n",
    "    # 1 - left\n",
    "    # 2 - stay still/  nothing\n",
    "    # 3 - right\n",
    "    # 4 - down\n",
    "    # return 4\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiger_policy(model, observation, prev_observation, no_previous=True):\n",
    "    global exploration_rate, device\n",
    "    observation.to(device=device)\n",
    "    if no_previous:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    # return torch.randint(0,model.n_actions, (1,))\n",
    "    # stacked_observations = torch.cat((observation, prev_observation), 1)\n",
    "    # Make a prediction based on the stacked observations\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    q_values = model.forward(observation)    \n",
    "    # Choose the action with the highest Q-value\n",
    "    action = torch.argmax(q_values)\n",
    "    # print(observation)\n",
    "    # 0 - up\n",
    "    # 1 - left\n",
    "    # 2 - stay still/  nothing\n",
    "    # 3 - right\n",
    "    # 4 - down\n",
    "    # 5 - attack up\n",
    "    # 6 - attack left\n",
    "    # 7 - attack right\n",
    "    # 8 - attack down\n",
    "    # return 5\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "# n_actions = env.action_space.n\n",
    "n_actions = 5\n",
    "# Get the number of state observations\n",
    "# state, info = env.reset()\n",
    "n_observations = (3,3,5)\n",
    "\n",
    "\n",
    "DQN_Basic((3,3,5), 5)\n",
    "\n",
    "\n",
    "policy_net = DQN_Basic(n_observations, n_actions).to(device) # What is this line for?\n",
    "target_net = DQN_Basic(n_observations, n_actions).to(device) # Tiger or Deer?\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# memory = ReplayMemory(10000) # is this line needed?\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(episode_data, model):\n",
    "    if len(episode_data) < batch_size:\n",
    "        return\n",
    "    optimizer.zero_grad()\n",
    "    batch = random.sample(episode_data, batch_size)\n",
    "    # [agent, observation, reward, done, info, action]\n",
    "    # [agent, prev_observation, observation, reward, done, info, action]\n",
    "    agent_batch, prev_obs_batch, obs_batch, reward_batch, done_batch, info_batch, action_batch = map(np.array, zip(*batch))\n",
    "    # print(type(prev_obs_batch))\n",
    "    # print(prev_obs_batch.shape)\n",
    "    # print(prev_obs_batch)\n",
    "\n",
    "    temp_obs = torch.zeros((prev_obs_batch.shape[0],3,3,5),device=device)\n",
    "    for index, x in enumerate(prev_obs_batch):\n",
    "        temp_obs[index,:,:,:] = x\n",
    "    prev_obs_batch = temp_obs\n",
    "\n",
    "    temp_obs = torch.zeros((obs_batch.shape[0],3,3,5),device=device)\n",
    "    for index, x in enumerate(obs_batch):\n",
    "        temp_obs[index,:,:,:] = x\n",
    "    obs_batch = temp_obs\n",
    "\n",
    "\n",
    "    # Find a way to add the masked observations to the state_batch\n",
    "    # state_batch = torch.cat((state_batch, masked_observations), 1)\n",
    "    # next_state_batch = torch.cat((next_state_batch, masked_observations), 1)\n",
    "    reward_batch = torch.tensor(reward_batch, dtype=torch.float32, device=device)\n",
    "    done_batch = torch.tensor(done_batch, dtype=torch.bool, device=device)\n",
    "    # info_batch = torch.tensor(info_batch, dtype=torch.float32, device=device)\n",
    "    action_batch = torch.tensor(action_batch, dtype=torch.int64, device=device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    # [0.1, 0.34, 0.4, 0.5]\n",
    "    # state_action_values = torch.argmax(model(prev_obs_batch),dim=1)\n",
    "    # Compute Q(s_t, a)\n",
    "    q_values = model(prev_obs_batch)\n",
    "    state_action_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_q_values = model(obs_batch).detach() # Detach to avoid computing gradients\n",
    "    next_state_values[done_batch == 0] = next_q_values[done_batch == 0].max(1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.detach())\n",
    "    print(\"Loss: \",loss)\n",
    "\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(episode_data):\n",
    "    # number of deer\n",
    "    # number of tigers\n",
    "    # average life?\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_observations(agent,agents_prev_data,observation):\n",
    "    maskDeer=torch.tensor([ [1,1,1],\n",
    "                            [1,1,1],\n",
    "                            [0,0,0],\n",
    "                            ])\n",
    "    deerOpt=[0,1,0,3,2]\n",
    "    maskTiger=torch.tensor([[1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            ])\n",
    "    tigerOpt=[0,1,0,3,2,0,1,3,2]\n",
    "    rotCount=0\n",
    "    if 'tiger' in agent:\n",
    "        if agents_prev_data[agent][5] is None:\n",
    "            rotCount=0\n",
    "        else:\n",
    "            rotCount=tigerOpt[agents_prev_data[agent][5]]\n",
    "        finalMask=maskTiger\n",
    "        maskedObs=observation\n",
    "        for i in range(rotCount):\n",
    "            finalMask=torch.rot90(finalMask, 1, [0, 1])\n",
    "        for i in range(observation.shape[3]):\n",
    "            maskedObs[:,:,:,i]=maskedObs[:,:,:,i]*finalMask\n",
    "        return maskedObs\n",
    "    else:\n",
    "        if agents_prev_data[agent][5] is None:\n",
    "            rotCount=0\n",
    "        else:\n",
    "            rotCount=deerOpt[agents_prev_data[agent][5]]\n",
    "        finalMask=maskDeer\n",
    "        maskedObs=observation\n",
    "        for i in range(rotCount):\n",
    "            finalMask=torch.rot90(finalMask, 1, [0, 1])\n",
    "        for i in range(observation.shape[3]):\n",
    "            maskedObs[:,:,:,i]=maskedObs[:,:,:,i]*finalMask\n",
    "        return maskedObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_type_from_agent_name(agent):\n",
    "    if 'tiger' in agent:\n",
    "        return 'tiger'\n",
    "    return 'deer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29984\\2817206283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtiger_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtiger_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_previous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_previous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeer_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeer_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_previous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_previous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29984\\1306166851.py\u001b[0m in \u001b[0;36mdeer_policy\u001b[1;34m(model, observation, prev_observation, no_previous)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mexploration_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Choose the action with the highest Q-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\OneDrive - Georgia Institute of Technology\\Georgia Tech\\GTSR\\Underwater Platforms\\Python_Code\\TheLearningShoal\\The-Learning-Shoal\\src\\tests\\dqn_basic.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# (1, H, W, C)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolve_and_rearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# rearrange input shape to (C, H, W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Scott\\a3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "tiger_model = DQN_Basic((9,9,5), 9)\n",
    "deer_model = DQN_Basic((3,3,5), 5)\n",
    "tiger_model.to(device=device)\n",
    "deer_model.to(device=device)\n",
    "agents_prev_data = {}\n",
    "for episodes in range(total_episodes):\n",
    "    env.reset(seed=None)\n",
    "    episode_data = {'deer': [], 'tiger':[]}\n",
    "    for agent in env.agent_iter():\n",
    "\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        observation = torch.unsqueeze(torch.from_numpy(observation),0)\n",
    "        observation.to(device=device)\n",
    "        \n",
    "        # instantiate previous data\n",
    "        no_previous = False\n",
    "        if agent not in agents_prev_data.keys():\n",
    "            agents_prev_data[agent] = [None]*6\n",
    "            no_previous = True\n",
    "\n",
    "        prev_observation = agents_prev_data[agent][0]\n",
    "        # print(prev_observation)\n",
    "\n",
    "        # set agent type to tiger or deer based on agentName\n",
    "        agentType = get_agent_type_from_agent_name(agent)\n",
    "        # print(observation[:,:,:,0])\n",
    "        # print(mask_observations(agent,agents_prev_data,observation)[:,:,:,0])\n",
    "        # input()\n",
    "        # catch if agent is dead\n",
    "        done = termination or truncation\n",
    "\n",
    "        if not done:\n",
    "            # if the agent is not dead\n",
    "            if 'tiger' in agent:\n",
    "                action =  tiger_policy(tiger_model, observation, prev_observation, no_previous=no_previous)\n",
    "            else:\n",
    "                action = deer_policy(deer_model, observation, prev_observation, no_previous=no_previous)\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.item()\n",
    "            if not no_previous:\n",
    "                episode_data[agentType].append([agent, prev_observation, observation, reward, done, info, action])\n",
    "        else:\n",
    "            # if agent is dead\n",
    "            action = None\n",
    "        \n",
    "        # previous recorded data\n",
    "        agents_prev_data[agent] = [observation, reward, termination, truncation, info, action]\n",
    "        env.step(action)\n",
    "\n",
    "    #########################\n",
    "    # plots for data\n",
    "    generate_plots(episode_data)\n",
    "    #########################\n",
    "    optimize_model(episode_data['deer'], deer_model)\n",
    "    # optimize_model(episode_data['tiger'], tiger_model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(type(env))\n",
    "print(env.num_agents)\n",
    "print(env.possible_agents)\n",
    "print(env.action_spaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
