{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4, adversarial_pursuit_v4, tiger_deer_v4\n",
    "import sys, os\n",
    "from pettingzoo.utils import random_demo\n",
    "import pygame\n",
    "import torch\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from dqn_basic import DQN_Basic\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = 'cpu'\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 4\n",
    "frame_size = (128, 128)\n",
    "max_cycles = 100\n",
    "total_episodes = 100\n",
    "map_size = 30\n",
    "exploration_rate=0.2 #[0,1]\n",
    "\n",
    "# env = tiger_deer_v4.env(map_size=map_size, minimap_mode=False, render_mode='rgb_array', tiger_step_recover=-0.1, deer_attacked=-0.1, max_cycles=max_cycles, extra_features=False)\n",
    "env = tiger_deer_v4.env(map_size=map_size, minimap_mode=False, render_mode='human', tiger_step_recover=-0.1, deer_attacked=-0.1, max_cycles=max_cycles, extra_features=False)\n",
    "\n",
    "# random_demo(env, render=False , episodes=1)\n",
    "# 'rgb_array'\n",
    "# pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiger_DQN_Basic(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(Tiger_DQN_Basic, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deer_DQN_Basic(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(Deer_DQN_Basic, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deer_policy(model, observation, prev_observation, no_previous=True):\n",
    "    global exploration_rate, device\n",
    "    observation.to(device=device)\n",
    "    \n",
    "    if no_previous:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    # return torch.randint(0,model.n_actions, (1,))\n",
    "    # stacked_observations = torch.cat((observation, prev_observation), 1)\n",
    "    # Make a prediction based on the stacked observations\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    q_values = model.forward(observation)    \n",
    "    # Choose the action with the highest Q-value\n",
    "    action = torch.argmax(q_values)\n",
    "    # print(type(observation))\n",
    "    # print(observation.shape)\n",
    "    # 0 - up\n",
    "    # 1 - left\n",
    "    # 2 - stay still/  nothing\n",
    "    # 3 - right\n",
    "    # 4 - down\n",
    "    # return 4\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiger_policy(model, observation, prev_observation, no_previous=True):\n",
    "    global exploration_rate, device\n",
    "    observation.to(device=device)\n",
    "    if no_previous:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    # return torch.randint(0,model.n_actions, (1,))\n",
    "    # stacked_observations = torch.cat((observation, prev_observation), 1)\n",
    "    # Make a prediction based on the stacked observations\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return torch.randint(0,model.n_actions, (1,))\n",
    "    q_values = model.forward(observation)    \n",
    "    # Choose the action with the highest Q-value\n",
    "    action = torch.argmax(q_values)\n",
    "    # print(observation)\n",
    "    # 0 - up\n",
    "    # 1 - left\n",
    "    # 2 - stay still/  nothing\n",
    "    # 3 - right\n",
    "    # 4 - down\n",
    "    # 5 - attack up\n",
    "    # 6 - attack left\n",
    "    # 7 - attack right\n",
    "    # 8 - attack down\n",
    "    # return 5\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "# n_actions = env.action_space.n\n",
    "n_actions = 5\n",
    "# Get the number of state observations\n",
    "# state, info = env.reset()\n",
    "n_observations = (3,3,5)\n",
    "\n",
    "\n",
    "DQN_Basic((3,3,5), 5)\n",
    "\n",
    "\n",
    "policy_net = DQN_Basic(n_observations, n_actions).to(device) # What is this line for?\n",
    "target_net = DQN_Basic(n_observations, n_actions).to(device) # Tiger or Deer?\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# memory = ReplayMemory(10000) # is this line needed?\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(episode_data, model):\n",
    "    if len(episode_data) < batch_size:\n",
    "        return\n",
    "    optimizer.zero_grad()\n",
    "    batch = random.sample(episode_data, batch_size)\n",
    "    # [agent, observation, reward, done, info, action]\n",
    "    # [agent, prev_observation, observation, reward, done, info, action]\n",
    "    agent_batch, prev_obs_batch, obs_batch, reward_batch, done_batch, info_batch, action_batch = map(np.array, zip(*batch))\n",
    "    # print(type(prev_obs_batch))\n",
    "    # print(prev_obs_batch.shape)\n",
    "    # print(prev_obs_batch)\n",
    "\n",
    "    temp_obs = torch.zeros((prev_obs_batch.shape[0],3,3,5),device=device)\n",
    "    for index, x in enumerate(prev_obs_batch):\n",
    "        temp_obs[index,:,:,:] = x\n",
    "    prev_obs_batch = temp_obs\n",
    "\n",
    "    temp_obs = torch.zeros((obs_batch.shape[0],3,3,5),device=device)\n",
    "    for index, x in enumerate(obs_batch):\n",
    "        temp_obs[index,:,:,:] = x\n",
    "    obs_batch = temp_obs\n",
    "\n",
    "\n",
    "    # Find a way to add the masked observations to the state_batch\n",
    "    # state_batch = torch.cat((state_batch, masked_observations), 1)\n",
    "    # next_state_batch = torch.cat((next_state_batch, masked_observations), 1)\n",
    "    reward_batch = torch.tensor(reward_batch, dtype=torch.float32, device=device)\n",
    "    done_batch = torch.tensor(done_batch, dtype=torch.bool, device=device)\n",
    "    # info_batch = torch.tensor(info_batch, dtype=torch.float32, device=device)\n",
    "    action_batch = torch.tensor(action_batch, dtype=torch.int64, device=device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    # [0.1, 0.34, 0.4, 0.5]\n",
    "    # state_action_values = torch.argmax(model(prev_obs_batch),dim=1)\n",
    "    # Compute Q(s_t, a)\n",
    "    q_values = model(prev_obs_batch)\n",
    "    state_action_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    # freezen net\n",
    "    next_q_values = model(obs_batch).detach() # Detach to avoid computing gradients\n",
    "    \n",
    "    next_state_values[done_batch == 0] = next_q_values[done_batch == 0].max(1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.detach())\n",
    "    print(\"Loss: \",loss)\n",
    "\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(episode_data):\n",
    "    # number of deer\n",
    "    # number of tigers\n",
    "    # average life?\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_observations(agent,agents_prev_data,observation):\n",
    "    maskDeer=torch.tensor([ [1,1,1],\n",
    "                            [1,1,1],\n",
    "                            [0,0,0],\n",
    "                            ])\n",
    "    deerOpt=[0,1,0,3,2]\n",
    "    maskTiger=torch.tensor([[1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [1,1,1,1,1,1,1,1,1],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0,0,0,0],\n",
    "                            ])\n",
    "    tigerOpt=[0,1,0,3,2,0,1,3,2]\n",
    "    rotCount=0\n",
    "    if 'tiger' in agent:\n",
    "        if agents_prev_data[agent][5] is None:\n",
    "            rotCount=0\n",
    "        else:\n",
    "            rotCount=tigerOpt[agents_prev_data[agent][5]]\n",
    "        finalMask=maskTiger\n",
    "        maskedObs=observation\n",
    "        for i in range(rotCount):\n",
    "            finalMask=torch.rot90(finalMask, 1, [0, 1])\n",
    "        for i in range(observation.shape[3]):\n",
    "            maskedObs[:,:,:,i]=maskedObs[:,:,:,i]*finalMask\n",
    "        return maskedObs\n",
    "    else:\n",
    "        if agents_prev_data[agent][5] is None:\n",
    "            rotCount=0\n",
    "        else:\n",
    "            rotCount=deerOpt[agents_prev_data[agent][5]]\n",
    "        finalMask=maskDeer\n",
    "        maskedObs=observation\n",
    "        for i in range(rotCount):\n",
    "            finalMask=torch.rot90(finalMask, 1, [0, 1])\n",
    "        for i in range(observation.shape[3]):\n",
    "            maskedObs[:,:,:,i]=maskedObs[:,:,:,i]*finalMask\n",
    "        return maskedObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_type_from_agent_name(agent):\n",
    "    if 'tiger' in agent:\n",
    "        return 'tiger'\n",
    "    return 'deer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TempSync\\CS7643\\The-Learning-Shoal\\src\\tests\\dqn_basic.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_58696\\4123188269.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  agent_batch, prev_obs_batch, obs_batch, reward_batch, done_batch, info_batch, action_batch = map(np.array, zip(*batch))\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_58696\\4123188269.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  agent_batch, prev_obs_batch, obs_batch, reward_batch, done_batch, info_batch, action_batch = map(np.array, zip(*batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  tensor(0.0002, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0003, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0004, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0002, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0005, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0004, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0006, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0004, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0004, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0002, grad_fn=<SmoothL1LossBackward0>)\n",
      "Loss:  tensor(0.0002, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\TempSync\\CS7643\\The-Learning-Shoal\\src\\tests\\tiger_deer_new.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     action \u001b[39m=\u001b[39m  tiger_policy(tiger_model, observation, prev_observation, no_previous\u001b[39m=\u001b[39mno_previous)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     action \u001b[39m=\u001b[39m deer_policy(deer_model, observation, prev_observation, no_previous\u001b[39m=\u001b[39;49mno_previous)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(action, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32md:\\TempSync\\CS7643\\The-Learning-Shoal\\src\\tests\\tiger_deer_new.ipynb Cell 12\u001b[0m in \u001b[0;36mdeer_policy\u001b[1;34m(model, observation, prev_observation, no_previous)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m exploration_rate:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,model\u001b[39m.\u001b[39mn_actions, (\u001b[39m1\u001b[39m,))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m q_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(observation)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Choose the action with the highest Q-value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TempSync/CS7643/The-Learning-Shoal/src/tests/tiger_deer_new.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(q_values)\n",
      "File \u001b[1;32md:\\TempSync\\CS7643\\The-Learning-Shoal\\src\\tests\\dqn_basic.py:31\u001b[0m, in \u001b[0;36mDQN_Basic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     \u001b[39m# (1, H, W, C)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvolve_and_rearrange(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))  \u001b[39m# rearrange input shape to (C, H, W)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked\u001b[39m.\u001b[39;49madd_(\u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    153\u001b[0m             exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tiger_model = DQN_Basic((9,9,5), 9)\n",
    "deer_model = DQN_Basic((3,3,5), 5)\n",
    "tiger_model.to(device=device)\n",
    "deer_model.to(device=device)\n",
    "agents_prev_data = {}\n",
    "for episodes in range(total_episodes):\n",
    "    env.reset(seed=None)\n",
    "    episode_data = {'deer': [], 'tiger':[]}\n",
    "    for agent in env.agent_iter():\n",
    "\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        observation = torch.unsqueeze(torch.from_numpy(observation),0)\n",
    "        observation.to(device=device)\n",
    "        \n",
    "        # instantiate previous data\n",
    "        no_previous = False\n",
    "        if agent not in agents_prev_data.keys():\n",
    "            agents_prev_data[agent] = [None]*6\n",
    "            no_previous = True\n",
    "\n",
    "        prev_observation = agents_prev_data[agent][0]\n",
    "        # print(prev_observation)\n",
    "\n",
    "        # set agent type to tiger or deer based on agentName\n",
    "        agentType = get_agent_type_from_agent_name(agent)\n",
    "        # print(observation[:,:,:,0])\n",
    "        # print(mask_observations(agent,agents_prev_data,observation)[:,:,:,0])\n",
    "        # input()\n",
    "        # catch if agent is dead\n",
    "        done = termination or truncation\n",
    "\n",
    "        if not done:\n",
    "            # if the agent is not dead\n",
    "            if 'tiger' in agent:\n",
    "                action =  tiger_policy(tiger_model, observation, prev_observation, no_previous=no_previous)\n",
    "            else:\n",
    "                action = deer_policy(deer_model, observation, prev_observation, no_previous=no_previous)\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.item()\n",
    "            if not no_previous:\n",
    "                episode_data[agentType].append([agent, prev_observation, observation, reward, done, info, action])\n",
    "        else:\n",
    "            # if agent is dead\n",
    "            action = None\n",
    "        \n",
    "        # previous recorded data\n",
    "        agents_prev_data[agent] = [observation, reward, termination, truncation, info, action]\n",
    "        env.step(action)\n",
    "\n",
    "    #########################\n",
    "    # plots for data\n",
    "    generate_plots(episode_data)\n",
    "    #########################\n",
    "    optimize_model(episode_data['deer'], deer_model)\n",
    "    # optimize_model(episode_data['tiger'], tiger_model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(type(env))\n",
    "print(env.num_agents)\n",
    "print(env.possible_agents)\n",
    "print(env.action_spaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
