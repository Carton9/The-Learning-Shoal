{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4, adversarial_pursuit_v4\n",
    "import sys, os\n",
    "from pettingzoo.utils import random_demo\n",
    "import pygame\n",
    "import torch\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "mymodule_dir = os.path.join( script_dir, '..', 'local_magent2', 'environments' )\n",
    "sys.path.append( mymodule_dir )\n",
    "\n",
    "import single_fish_shoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.    -0.005 -0.1   -0.1    0.2  ]\n",
      "Average total reward -69.80000246278942\n"
     ]
    }
   ],
   "source": [
    "env = single_fish_shoal.env(render_mode='human',max_cycles=10)\n",
    "random_demo(env, render=False , episodes=10)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseWrapper.action_space of <pettingzoo.utils.wrappers.order_enforcing.OrderEnforcingWrapper object at 0x00000150AACE5F40>>\n",
      "<class 'pettingzoo.utils.wrappers.order_enforcing.OrderEnforcingWrapper'>\n",
      "0\n",
      "['red_0', 'red_1', 'red_2', 'red_3', 'red_4', 'red_5', 'red_6', 'red_7', 'red_8', 'red_9', 'red_10', 'red_11', 'red_12', 'red_13', 'red_14', 'red_15', 'red_16', 'red_17', 'red_18', 'red_19', 'red_20', 'red_21', 'red_22', 'red_23', 'red_24', 'red_25', 'red_26', 'red_27', 'red_28', 'red_29', 'red_30', 'red_31', 'red_32', 'red_33', 'red_34', 'red_35', 'red_36', 'red_37', 'red_38', 'red_39', 'red_40', 'red_41', 'red_42', 'red_43', 'red_44', 'red_45', 'red_46', 'red_47', 'red_48', 'red_49', 'red_50', 'red_51', 'red_52', 'red_53', 'red_54', 'red_55', 'red_56', 'red_57', 'red_58', 'red_59', 'red_60', 'red_61', 'red_62', 'red_63', 'red_64', 'red_65', 'red_66', 'red_67', 'red_68', 'red_69', 'red_70', 'red_71', 'red_72', 'red_73', 'red_74', 'red_75', 'red_76', 'red_77', 'red_78', 'red_79', 'red_80', 'blue_0', 'blue_1', 'blue_2', 'blue_3', 'blue_4', 'blue_5', 'blue_6', 'blue_7', 'blue_8', 'blue_9', 'blue_10', 'blue_11', 'blue_12', 'blue_13', 'blue_14', 'blue_15', 'blue_16', 'blue_17', 'blue_18', 'blue_19', 'blue_20', 'blue_21', 'blue_22', 'blue_23', 'blue_24', 'blue_25', 'blue_26', 'blue_27', 'blue_28', 'blue_29', 'blue_30', 'blue_31', 'blue_32', 'blue_33', 'blue_34', 'blue_35', 'blue_36', 'blue_37', 'blue_38', 'blue_39', 'blue_40', 'blue_41', 'blue_42', 'blue_43', 'blue_44', 'blue_45', 'blue_46', 'blue_47', 'blue_48', 'blue_49', 'blue_50', 'blue_51', 'blue_52', 'blue_53', 'blue_54', 'blue_55', 'blue_56', 'blue_57', 'blue_58', 'blue_59', 'blue_60', 'blue_61', 'blue_62', 'blue_63', 'blue_64', 'blue_65', 'blue_66', 'blue_67', 'blue_68', 'blue_69', 'blue_70', 'blue_71', 'blue_72', 'blue_73', 'blue_74', 'blue_75', 'blue_76', 'blue_77', 'blue_78', 'blue_79', 'blue_80']\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(type(env))\n",
    "print(env.num_agents)\n",
    "print(env.possible_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 4\n",
    "frame_size = (64, 64)\n",
    "max_cycles = 125\n",
    "total_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation, agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red_0': 0, 'red_1': 0, 'red_2': 0, 'red_3': 0, 'red_4': 0, 'red_5': 0, 'red_6': 0, 'red_7': 0, 'red_8': 0, 'red_9': 0, 'red_10': 0, 'red_11': 0, 'red_12': 0, 'red_13': 0, 'red_14': 0, 'red_15': 0, 'red_16': 0, 'red_17': 0, 'red_18': 0, 'red_19': 0, 'red_20': 0, 'red_21': 0, 'red_22': 0, 'red_23': 0, 'red_24': 0, 'red_25': 0, 'red_26': 0, 'red_27': 0, 'red_28': 0, 'red_29': 0, 'red_30': 0, 'red_31': 0, 'red_32': 0, 'red_33': 0, 'red_34': 0, 'red_35': 0, 'red_36': 0, 'red_37': 0, 'red_38': 0, 'red_39': 0, 'red_40': 0, 'red_41': 0, 'red_42': 0, 'red_43': 0, 'red_44': 0, 'red_45': 0, 'red_46': 0, 'red_47': 0, 'red_48': 0, 'red_49': 0, 'red_50': 0, 'red_51': 0, 'red_52': 0, 'red_53': 0, 'red_54': 0, 'red_55': 0, 'red_56': 0, 'red_57': 0, 'red_58': 0, 'red_59': 0, 'red_60': 0, 'red_61': 0, 'red_62': 0, 'red_63': 0, 'red_64': 0, 'red_65': 0, 'red_66': 0, 'red_67': 0, 'red_68': 0, 'red_69': 0, 'red_70': 0, 'red_71': 0, 'red_72': 0, 'red_73': 0, 'red_74': 0, 'red_75': 0, 'red_76': 0, 'red_77': 0, 'red_78': 0, 'red_79': 0, 'red_80': 0, 'blue_0': 0, 'blue_1': 0, 'blue_2': 0, 'blue_3': 0, 'blue_4': 0, 'blue_5': 0, 'blue_6': 0, 'blue_7': 0, 'blue_8': 0, 'blue_9': 0, 'blue_10': 0, 'blue_11': 0, 'blue_12': 0, 'blue_13': 0, 'blue_14': 0, 'blue_15': 0, 'blue_16': 0, 'blue_17': 0, 'blue_18': 0, 'blue_19': 0, 'blue_20': 0, 'blue_21': 0, 'blue_22': 0, 'blue_23': 0, 'blue_24': 0, 'blue_25': 0, 'blue_26': 0, 'blue_27': 0, 'blue_28': 0, 'blue_29': 0, 'blue_30': 0, 'blue_31': 0, 'blue_32': 0, 'blue_33': 0, 'blue_34': 0, 'blue_35': 0, 'blue_36': 0, 'blue_37': 0, 'blue_38': 0, 'blue_39': 0, 'blue_40': 0, 'blue_41': 0, 'blue_42': 0, 'blue_43': 0, 'blue_44': 0, 'blue_45': 0, 'blue_46': 0, 'blue_47': 0, 'blue_48': 0, 'blue_49': 0, 'blue_50': 0, 'blue_51': 0, 'blue_52': 0, 'blue_53': 0, 'blue_54': 0, 'blue_55': 0, 'blue_56': 0, 'blue_57': 0, 'blue_58': 0, 'blue_59': 0, 'blue_60': 0, 'blue_61': 0, 'blue_62': 0, 'blue_63': 0, 'blue_64': 0, 'blue_65': 0, 'blue_66': 0, 'blue_67': 0, 'blue_68': 0, 'blue_69': 0, 'blue_70': 0, 'blue_71': 0, 'blue_72': 0, 'blue_73': 0, 'blue_74': 0, 'blue_75': 0, 'blue_76': 0, 'blue_77': 0, 'blue_78': 0, 'blue_79': 0, 'blue_80': 0}\n",
      "[[[1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24768\\2709172469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# print(reward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# print(info)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;31m# action = policy(observation, agent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m# env.step(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'action'"
     ]
    }
   ],
   "source": [
    "for episode in range(total_episodes):\n",
    "    with torch.no_grad():\n",
    "        env.reset()\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            print(env.rewards)\n",
    "            print(env.unwrapped.base_state)\n",
    "            # print(observation)\n",
    "            # print(reward)\n",
    "            # print(info)\n",
    "            env.step()\n",
    "            # action = policy(observation, agent)\n",
    "            # env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pistonball_v6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1616\\1962759628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;34m\"\"\" ENV SETUP \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     env = pistonball_v6.parallel_env(\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mrender_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_cycles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_cycles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pistonball_v6' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    stack_size = 4\n",
    "    frame_size = (64, 64)\n",
    "    max_cycles = 125\n",
    "    total_episodes = 2\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles\n",
    "    )\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    # train for n number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # collect observations and convert to batch of torch tensors\n",
    "            next_obs = env.reset(seed=None)\n",
    "            # reset the episodic return\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "\n",
    "                # rollover the observation\n",
    "                obs = batchify_obs(next_obs, device)\n",
    "\n",
    "                # get action from the agent\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "\n",
    "                # execute the environment and log data\n",
    "                next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                    unbatchify(actions, env)\n",
    "                )\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = obs\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                # compute episodic return\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    end_step = step\n",
    "                    break\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            for t in reversed(range(end_step)):\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        # convert our episodes to batch of individual transitions\n",
    "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(3):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantaegs\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    \"\"\" RENDER THE POLICY \"\"\"\n",
    "    env = pistonball_v6.parallel_env(render_mode=\"human\", continuous=False)\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, 64, 64)\n",
    "    env = frame_stack_v1(env, stack_size=4)\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # render 5 episodes out\n",
    "        for episode in range(total_episodes):\n",
    "            print(\"Displaying Episode: \", episode)\n",
    "            obs = batchify_obs(env.reset(seed=None), device)\n",
    "            terms = [False]\n",
    "            truncs = [False]\n",
    "            while not any(terms) and not any(truncs):\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "                obs = batchify_obs(obs, device)\n",
    "                terms = [terms[a] for a in terms]\n",
    "                truncs = [truncs[a] for a in truncs]\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
